{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0aea72c8-2640-42b1-9dc1-7de6fb5eb1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using text column: Date\n",
      "Preprocessing text...\n",
      "Sample cleaned text:\n",
      "0    \n",
      "1    \n",
      "2    \n",
      "3    \n",
      "4    \n",
      "Name: clean_text, dtype: object\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 100\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# TF-IDF VECTORIZATION\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# ---------------------------------------------------------\u001b[39;00m\n\u001b[0;32m     99\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000\u001b[39m)\n\u001b[1;32m--> 100\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean_text\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    102\u001b[0m tfidf_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m    103\u001b[0m     tfidf_matrix\u001b[38;5;241m.\u001b[39mtoarray(),\n\u001b[0;32m    104\u001b[0m     columns\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m    105\u001b[0m )\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Save output\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:2104\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2098\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2099\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2100\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2101\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2102\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2103\u001b[0m )\n\u001b[1;32m-> 2104\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit_transform(raw_documents)\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2106\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2107\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m             )\n\u001b[0;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_count_vocab(raw_documents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfixed_vocabulary_)\n\u001b[0;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda4\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:1282\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1283\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1284\u001b[0m         )\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# COMPLETE NLP PREPROCESSING + TF-IDF FOR CSV FILE\n",
    "# Works with: FINAL_USO_cleaned.csv\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "!pip install -q nltk scikit-learn pandas\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# LOAD CSV (YOUR FILE)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "FILE_PATH = \"FINAL_USO_cleaned.csv\"   # <-- Correct file name\n",
    "\n",
    "if not os.path.isfile(FILE_PATH):\n",
    "    raise FileNotFoundError(f\"File not found: {FILE_PATH}. \"\n",
    "                            \"Make sure FINAL_USO_cleaned.csv is in the same folder as your notebook.\")\n",
    "\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# FIND TEXT COLUMN\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Try to find a usable text column automatically\n",
    "possible_text_cols = [\n",
    "    \"text\", \"comments\", \"description\", \"body\", \"content\", \n",
    "    \"article\", \"review\", \"notes\", \"document\"\n",
    "]\n",
    "\n",
    "text_col = None\n",
    "\n",
    "for col in df.columns:\n",
    "    if col.lower() in possible_text_cols:\n",
    "        text_col = col\n",
    "        break\n",
    "\n",
    "# If no obvious text column, fall back to first string column\n",
    "if text_col is None:\n",
    "    string_cols = [c for c in df.columns if df[c].dtype == object]\n",
    "    if len(string_cols) == 0:\n",
    "        raise ValueError(\n",
    "            \"No text column found in CSV. \"\n",
    "            \"Please add a column containing text or tell me the correct column name.\"\n",
    "        )\n",
    "    text_col = string_cols[0]\n",
    "\n",
    "print(f\"Using text column: {text_col}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# PREPROCESSING FUNCTION\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha()]               # keep only letters\n",
    "    tokens = [t for t in tokens if t not in stop_words]       # remove stopwords\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]        # lemmatize\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# APPLY PREPROCESSING\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "print(\"Preprocessing text...\")\n",
    "\n",
    "df[\"clean_text\"] = df[text_col].astype(str).apply(preprocess_text)\n",
    "\n",
    "print(\"Sample cleaned text:\")\n",
    "print(df[\"clean_text\"].head())\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# TF-IDF VECTORIZATION\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=2000)\n",
    "tfidf_matrix = vectorizer.fit_transform(df[\"clean_text\"])\n",
    "\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# Save output\n",
    "tfidf_df.to_csv(\"tfidf_output.csv\", index=False)\n",
    "\n",
    "print(\"\\nTF-IDF shape:\", tfidf_df.shape)\n",
    "print(\"Saved TF-IDF matrix to tfidf_output.csv\\n\")\n",
    "\n",
    "# Show first few features\n",
    "print(\"Sample TF-IDF columns:\")\n",
    "print(tfidf_df.iloc[:, :20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d410855-1833-4169-886c-7fcfc37016bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47763b6-ff80-46c5-b2fa-afaa54485c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
